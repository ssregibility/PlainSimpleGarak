# Plain Simple Garak

This work proposes a recursive parameter-sharing structure and training mechanism for ConvNets.
- In the proposed ConvNet architecture, convolution layers are decomposed into a filter basis, that can be shared recursively, and layer-specific parts.
- We conjecture that a shared filter basis combined with a small amount of layer-specific parameters can retain, or further enhance, the representation power of individual layers, if a proper training method is applied. 

## Requirements

- Python 3.6.9
- pytorch 1.7.0+
- CUDA 10.1+

To install requirements:

```setup
pip3 install -r requirements.txt
```

## Model Training

NOT IMPLEMENTED YET, USE PlainSimpleGarak.ipynb INSTEAD

## Text Generation

NOT IMPLEMENTED YET, USE PlainSimpleGarak.ipynb INSTEAD

## Results

TODO

## Contributing

There is no way to contribute to the code yet - however, this is subject to be changed.
