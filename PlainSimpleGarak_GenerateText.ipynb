{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PlainSimpleGarak_GenerateText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOsnqr6kCWmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4863a71c-bed9-453e-f30a-0da7169e8994"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "HOME = 'drive/MyDrive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjqtb-uCWuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2df61c34-11ce-40cc-9507-2018cddeadcb"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import os"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brc2YaVkCW0X"
      },
      "source": [
        "#Encoding texts to indices\r\n",
        "def encode(string, word2index):\r\n",
        "  return torch.LongTensor([[word2index[wd] for wd in nltk.word_tokenize(string)]])\r\n",
        "\r\n",
        "#Decoding indices to texts\r\n",
        "def decode(vec, index2word):\r\n",
        "  return [index2word.get(x) for x in vec]\r\n",
        "\r\n",
        "x = torch.load('drive/MyDrive/saved_dicts')\r\n",
        "word2index = x['word2index']\r\n",
        "index2word = x['index2word']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzrtEhX9CW7z"
      },
      "source": [
        "class Net_variant(nn.Module):\r\n",
        "  def __init__(self, embed_size, input_dim, hidden_dim, batch_first=True, n_layers = 1, dropout = 0.2):\r\n",
        "    super(Net_variant, self).__init__()\r\n",
        "\r\n",
        "    self.n_layers = n_layers\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "    #shared embedding layer\r\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=embed_size, embedding_dim=input_dim)\r\n",
        "    \r\n",
        "    #GRU 1\r\n",
        "    self.rnn_layer1 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first, num_layers=n_layers, dropout=dropout, bidirectional=True)\r\n",
        "    self.linear1 = nn.Linear(hidden_dim*2, embed_size)\r\n",
        "\r\n",
        "    #GRU 2\r\n",
        "    self.rnn_layer2 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first, num_layers=n_layers, dropout=dropout, bidirectional=True)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim*2, embed_size)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.embedding_layer(x)\r\n",
        "\r\n",
        "    #Randomly selects which GRU layer should be used\r\n",
        "    if (random.randrange(2) == 0):\r\n",
        "      output1, hidden1 = self.rnn_layer1(output)\r\n",
        "      output1 = self.linear1(output1)\r\n",
        "      return output1\r\n",
        "    else:\r\n",
        "      output2, hidden2 = self.rnn_layer2(output)\r\n",
        "      output2 = self.linear2(output2)\r\n",
        "      return output2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djk2Wm4ECXNX"
      },
      "source": [
        "def test_model(model, word2index, index2word, string=\"\", maxlen=25, verbose=False):\r\n",
        "  #string is the input\r\n",
        "  #maxlen defines max length of the generated txt\r\n",
        "  #if verbose==True, shows every loops' input and output. if verbose==False, shows initial input and final output only.\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  eval_input = encode(string, word2index).cuda()\r\n",
        "  print(\"INITIAL INPUT: \" + string)\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "    print(\"---\")\r\n",
        "\r\n",
        "  for i in range(maxlen):\r\n",
        "    output = model(eval_input)\r\n",
        "    pred = output.softmax(-1).argmax(-1)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "      print(\"INPUT: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "      print(\"OUTPUT: \" + \" \".join( decode(pred[0].tolist(), index2word)))\r\n",
        "\r\n",
        "    eval_input = torch.cat((eval_input,pred[:,-1].unsqueeze(0)), 1)\r\n",
        "\r\n",
        "    if word2index['END'] in eval_input:\r\n",
        "      break\r\n",
        "\r\n",
        "  print(\"GENERATED SEQUENCE: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "  print(\"\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aKVYk9VbNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbab8d5-b698-4602-8d72-a92df6afe5fc"
      },
      "source": [
        "#Hyperparameters\r\n",
        "vocab_size = len(word2index)\r\n",
        "input_size =  128\r\n",
        "hidden_size = 128\r\n",
        "\r\n",
        "model = Net_variant(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "x = torch.load(\"drive/MyDrive/\" + \"CPOINT_FINETUNE-75\")\r\n",
        "model.load_state_dict(x['model_state_dict'])"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7Bvyt8ysqmw"
      },
      "source": [
        ""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTuY0mFsPGJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c1d4d8-5902-4dcb-f073-449b071224e6"
      },
      "source": [
        "####################################################################################################################################################################################\r\n",
        "\r\n",
        "# ENTER YOUR INPUT IN HERE\r\n",
        "#EXAMPLE: input_string = \"I am not a spy.\"\r\n",
        "\r\n",
        "# Token X no in the dict. means the input can't be used - change the token(word) to something else.\r\n",
        "\r\n",
        "####################################################################################################################################################################################\r\n",
        "\r\n",
        "#Change here\r\n",
        "input_string = \"My dear doctor, \"\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "input_string = input_string.lower()\r\n",
        "input_tokens = nltk.word_tokenize(input_string)\r\n",
        "flag_generate = True\r\n",
        "for token in input_tokens:\r\n",
        "  if token not in word2index:\r\n",
        "    print(\"Token [ {} ] not in the dict.\".format(token))\r\n",
        "    flag_generate = False\r\n",
        "\r\n",
        "if flag_generate:\r\n",
        "  test_model(model, word2index, index2word, \" \".join(input_tokens))"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INITIAL INPUT: my dear doctor ,\n",
            "GENERATED SEQUENCE: my dear doctor , i 'm sure the council members is motivated strictly by patriotism . END\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BR4yGzIlYkPb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hX7I6J_vYkXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VxdecADjYkkc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}