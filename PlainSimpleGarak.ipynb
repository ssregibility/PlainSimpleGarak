{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlainSimpleGarak.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOsnqr6kCWmk"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "HOME = 'drive/MyDrive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjqtb-uCWuJ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brc2YaVkCW0X"
      },
      "source": [
        "dataset_quotes = []\r\n",
        "\r\n",
        "import os\r\n",
        "with open(\"drive/MyDrive/Data/PlainSimpleGarak-data.txt\", 'r') as f: # open in readonly mode\r\n",
        "  while True:\r\n",
        "    line = f.readline().strip()\r\n",
        "    if line == '':\r\n",
        "        break\r\n",
        "    else:\r\n",
        "      dataset_quotes.append(line)\r\n",
        "\r\n",
        "dataset_startwords = []\r\n",
        "for s in dataset_quotes:\r\n",
        "  dataset_startwords.append(s.split()[0].lower())\r\n",
        "\r\n",
        "dataset_raw = pd.read_csv('drive/MyDrive/Data/quotes.csv', error_bad_lines=False, engine='python')\r\n",
        "\r\n",
        "dataset_quotes_nonascii = list(dataset_raw[\"Quote\"] )\r\n",
        "\r\n",
        "for s in dataset_quotes_nonascii:\r\n",
        "  if (len(s) == len(s.encode())):\r\n",
        "    if (\"$\" not in s and \"&\" not in s):\r\n",
        "      dataset_quotes.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzrtEhX9CW7z"
      },
      "source": [
        "def preprocess_txt(dataset_quotes, appende=True):\r\n",
        "  for i in range(len(dataset_quotes)):\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('\\t', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('--', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(' - ', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('%', ' percent')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('[', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(']', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(':', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(';', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('?', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('#', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('!', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('/', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('.', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(',', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('(', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(')', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('*', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('+', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('\\\"', '')\r\n",
        "    #dataset_quotes[i] = dataset_quotes[i].replace('\\'', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].lower()\r\n",
        "    if (appende == True):\r\n",
        "      dataset_quotes[i] = dataset_quotes[i] + \" <END>\"\r\n",
        "\r\n",
        "preprocess_txt(dataset_quotes)\r\n",
        "preprocess_txt(dataset_startwords, False)\r\n",
        "\r\n",
        "#dataset_quotes.sort()\r\n",
        "dataset_startwords = list(set(dataset_startwords))\r\n",
        "#dataset_startwords.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djk2Wm4ECXNX"
      },
      "source": [
        "def encode(string, word2index):\r\n",
        "  return torch.LongTensor([[word2index[wd] for wd in string.split()]])\r\n",
        "\r\n",
        "def decode(vec, index2word):\r\n",
        "  return [index2word.get(x) for x in vec]\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, txt, seq_len, word2index):\r\n",
        "    self.encoded = [word2index[wd] for wd in txt]\r\n",
        "    self.seq_len = seq_len\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.encoded) - self.seq_len\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    return ( torch.tensor(self.encoded[index:index+self.seq_len]), torch.tensor(self.encoded[index+1:index+self.seq_len+1]) )\r\n",
        "\r\n",
        "txt_quotes = \"\"\r\n",
        "for s in dataset_quotes:\r\n",
        "  txt_quotes = txt_quotes + \" \" + s\r\n",
        "txt_quotes = txt_quotes.split()\r\n",
        "\r\n",
        "list_words = list(set(txt_quotes))\r\n",
        "list_words.sort()\r\n",
        "\r\n",
        "word2index = {tkn: i for i, tkn in enumerate(list_words, 1)}\r\n",
        "word2index['<UNKNOWN>']=0\r\n",
        "index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aKVYk9VbNf"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "  def __init__(self, embed_size, input_dim, hidden_dim, batch_first=True, n_layers = 1, dropout = 0.2):\r\n",
        "    super(Net, self).__init__()\r\n",
        "\r\n",
        "    self.n_layers = n_layers #unused\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=embed_size, embedding_dim=input_dim)\r\n",
        "    #1\r\n",
        "    self.rnn_layer = nn.GRU(input_dim, hidden_dim, batch_first=batch_first, num_layers=n_layers, dropout=dropout)\r\n",
        "    #self.dropout = nn.Dropout(0.4)\r\n",
        "    self.linear = nn.Linear(hidden_dim, embed_size)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.embedding_layer(x)\r\n",
        "\r\n",
        "    output, hidden = self.rnn_layer(output)\r\n",
        "    #output = self.dropout(output)\r\n",
        "    output = self.linear(output)\r\n",
        "\r\n",
        "    return output\r\n",
        "    #return output.view(-1, output.size(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTuY0mFsPGJM"
      },
      "source": [
        "vocab_size = len(word2index)\r\n",
        "input_size =  128\r\n",
        "hidden_size = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIrDNq1PGbB"
      },
      "source": [
        "model1 = Net(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model1.cuda()\r\n",
        "model2 = Net(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model2.cuda()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer1 = optim.Adam(params=model1.parameters())\r\n",
        "optimizer2 = optim.Adam(params=model2.parameters())\r\n",
        "\r\n",
        "dataset = Dataset(txt_quotes, 5, word2index)\r\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3i1JU-yCaUM"
      },
      "source": [
        "epoch_to_load = 0\r\n",
        "\r\n",
        "if epoch_to_load != 0:\r\n",
        "  x = torch.load(\"drive/MyDrive/Data/Checkpoint1/\" + \"CPOINT-\" + str(epoch_to_load))\r\n",
        "  model1.load_state_dict(x['model1_state_dict'])\r\n",
        "  model2.load_state_dict(x['model2_state_dict'])\r\n",
        "  optimizer1.load_state_dict(x['optimizer1_state_dict'])\r\n",
        "  optimizer2.load_state_dict(x['optimizer2_state_dict'])\r\n",
        "  epoch_to_load = epoch_to_load + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ifOlYWPGhV"
      },
      "source": [
        "def test_model(model, word2index, index2word, string=\"\", maxlen=25, verbose=False):\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  eval_input = encode(string, word2index).cuda()\r\n",
        "  print(\"INITIAL INPUT: \" + string)\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "    print(\"---\")\r\n",
        "\r\n",
        "  for i in range(maxlen):\r\n",
        "    output = model(eval_input)\r\n",
        "    pred = output.softmax(-1).argmax(-1)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "      print(\"INPUT: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "      print(\"OUTPUT: \" + \" \".join( decode(pred[0].tolist(), index2word)))\r\n",
        "\r\n",
        "    eval_input = torch.cat((eval_input,pred[:,-1].unsqueeze(0)), 1)\r\n",
        "\r\n",
        "    if word2index['<END>'] in eval_input:\r\n",
        "      break\r\n",
        "\r\n",
        "  print(\"GENERATED SEQUENCE: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfHtYA-yPGnY"
      },
      "source": [
        "for epoch in range(epoch_to_load, 1001):\r\n",
        "  model1.train()\r\n",
        "  model2.train()\r\n",
        "\r\n",
        "  for batch, (input, target) in enumerate(dataloader):\r\n",
        "    optimizer1.zero_grad()\r\n",
        "    output1 = model1(input.cuda())\r\n",
        "    loss1 = criterion(output1.transpose(1, 2), target.cuda())\r\n",
        "    loss1.backward()\r\n",
        "    optimizer1.step()\r\n",
        "\r\n",
        "    optimizer2.zero_grad()\r\n",
        "    output2 = model2(input.cuda())\r\n",
        "    loss2 = criterion(output2.transpose(1, 2), target.cuda())\r\n",
        "    loss2.backward()\r\n",
        "    optimizer2.step()\r\n",
        "\r\n",
        "  model1.eval()\r\n",
        "  model2.eval()\r\n",
        "  print(\"Epoch {:02d} / 1001 Loss1 {:.4f} Loss2 {:.4f}\".format(epoch+1, loss1, loss2))\r\n",
        "  \r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output1[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"---------------------------------------------------------------------------\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output2[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "  test_model(model1, word2index, index2word, randword)\r\n",
        "  test_model(model1, word2index, index2word, \"i am plain simple garak \" + randword)\r\n",
        "\r\n",
        "  test_model(model2, word2index, index2word, randword)\r\n",
        "  test_model(model2, word2index, index2word, \"i am plain simple garak \" + randword)\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  if (epoch % 5 == 0):\r\n",
        "    #torch.save(model.state_dict(), \"drive/MyDrive/Data/\" + \"Checkpoint-\" + str(epoch) )\r\n",
        "    torch.save(\r\n",
        "        {'model1_state_dict': model1.state_dict(),\r\n",
        "         'model2_state_dict': model2.state_dict(),\r\n",
        "         'optimizer1_state_dict': optimizer1.state_dict(),\r\n",
        "         'optimizer2_state_dict': optimizer2.state_dict(),},\r\n",
        "        'drive/MyDrive/Data/Checkpoint1/' + \"CPOINT-\" + str(epoch)\r\n",
        "               )\r\n",
        "\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}