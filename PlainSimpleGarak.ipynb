{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PlainSimpleGarak.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPokyDfSz3NczCxtP7aIG0X"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TOsnqr6kCWmk"},"source":["from google.colab import drive\r\n","drive.mount(\"/content/drive\")\r\n","HOME = 'drive/MyDrive'\r\n","!pip install torchtext==0.4"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTjqtb-uCWuJ"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.optim as optim\r\n","\r\n","import random\r\n","import pandas as pd\r\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Brc2YaVkCW0X"},"source":["dataset_quotes = []\r\n","\r\n","import os\r\n","with open(\"drive/MyDrive/Data/PlainSimpleGarak-data.txt\", 'r') as f: # open in readonly mode\r\n","  while True:\r\n","    line = f.readline().strip()\r\n","    if line == '':\r\n","        break\r\n","    else:\r\n","      dataset_quotes.append(line)\r\n","\r\n","dataset_startwords = []\r\n","\r\n","\r\n","for s in dataset_quotes:\r\n","  dataset_startwords.append(s.split()[0].lower())\r\n","\r\n","dataset_raw = pd.read_csv('drive/MyDrive/Data/quotes.csv', error_bad_lines=False, engine='python')\r\n","\r\n","dataset_quotes_nonascii = list(dataset_raw[\"Quote\"] )\r\n","\r\n","for s in dataset_quotes_nonascii:\r\n","  if (len(s) == len(s.encode())):\r\n","    if (\"$\" not in s and \"&\" not in s):\r\n","      dataset_quotes.append(s)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzrtEhX9CW7z"},"source":["def preprocess_txt(dataset_quotes, appende=True):\r\n","  for i in range(len(dataset_quotes)):\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('\\t', ' ')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('--', ' ')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(' - ', ' ')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('%', ' percent')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('[', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(']', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(':', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(';', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('?', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('#', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('!', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('/', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('.', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(',', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('(', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace(')', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('*', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('+', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].replace('\\\"', '')\r\n","    #dataset_quotes[i] = dataset_quotes[i].replace('\\'', '')\r\n","    dataset_quotes[i] = dataset_quotes[i].lower()\r\n","    if (appende == True):\r\n","      dataset_quotes[i] = dataset_quotes[i] + \" END\"\r\n","\r\n","preprocess_txt(dataset_quotes)\r\n","preprocess_txt(dataset_startwords, False)\r\n","\r\n","dataset_startwords = list(set(dataset_startwords))\r\n","\r\n","random.shuffle(dataset_quotes)\r\n","random.shuffle(dataset_startwords)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"djk2Wm4ECXNX"},"source":["txt_quotes = \"\"\r\n","for s in dataset_quotes:\r\n","  txt_quotes = txt_quotes + \" \" + s\r\n","txt_quotes = txt_quotes.split()\r\n","\r\n","list_words = list(set(txt_quotes))\r\n","word2index = {tkn: i for i, tkn in enumerate(list_words, 1)}\r\n","word2index['<UNKNOWN>']=0\r\n","index2word = {v: k for k, v in word2index.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aPYA4ABfCXVU"},"source":["def string_to_trainpair(string, word2index):\r\n","  string = string.split()\r\n","  encoded = [word2index[wd] for wd in string]\r\n","  input_str, target_str = encoded[:-1], encoded[1:]\r\n","  input_str = torch.LongTensor(input_str).unsqueeze(0)\r\n","  target_str = torch.LongTensor(target_str).unsqueeze(0)\r\n","  return input_str, target_str\r\n","\r\n","def encode(string, word2index):\r\n","  return torch.LongTensor([[word2index[token] for token in string.split()]])\r\n","\r\n","def decode(vec, index2word):\r\n","  return [index2word.get(x) for x in vec]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3s8w7E_CXcf"},"source":["dataset_train = []\r\n","dataset_target = []\r\n","\r\n","for s in dataset_quotes:\r\n","  input, target = string_to_trainpair(s, word2index)\r\n","  dataset_train.append(input)\r\n","  dataset_target.append(target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1vR2qyCCXkR"},"source":["class Net(nn.Module):\r\n","  def __init__(self, embed_size, input_dim, hidden_dim, batch_first=True):\r\n","    super(Net, self).__init__()\r\n","\r\n","    self.n_layers = 1 #unused\r\n","    self.hidden_dim = hidden_dim\r\n","\r\n","    self.embedding_layer = nn.Embedding(num_embeddings=embed_size, embedding_dim=input_dim)\r\n","    #1\r\n","    self.rnn_layer1 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first)\r\n","    self.linear1 = nn.Linear(hidden_dim, embed_size)\r\n","    #2\r\n","    self.rnn_layer2 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first)\r\n","    self.linear2 = nn.Linear(hidden_dim, embed_size)\r\n","\r\n","  def forward(self, x):\r\n","    output = self.embedding_layer(x)\r\n","\r\n","    if self.training:\r\n","      output_1, hidden_1 = self.rnn_layer1(output)\r\n","      output_1 = self.linear1(output_1)\r\n","      output_2, hidden_2 = self.rnn_layer2(output)\r\n","      output_2 = self.linear2(output_2)\r\n","\r\n","      output = 0.5 * (output_1 + output_2)\r\n","\r\n","    else:\r\n","      if (random.randrange(2) == 0):\r\n","        output, hidden = self.rnn_layer1(output)\r\n","        output = self.linear1(output)\r\n","      else:\r\n","        output, hidden = self.rnn_layer2(output)\r\n","        output = self.linear2(output)\r\n","    return output.view(-1, output.size(2))\r\n","\r\n","  def init_hidden(self, batch_size):\r\n","      return torch.zeros(self.n_layers, batch_size, self.hidden_dim)  #unused"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTuY0mFsPGJM"},"source":["vocab_size = len(word2index)\r\n","input_size =  256\r\n","hidden_size = 512"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qkIrDNq1PGbB"},"source":["model = Net(vocab_size, input_size, hidden_size, batch_first=True)\r\n","model.cuda()\r\n","\r\n","criterion = nn.CrossEntropyLoss()\r\n","optimizer = optim.Adam(params=model.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6ifOlYWPGhV"},"source":["def test_model(model, word2index, index2word, string=\"\"):\r\n","  model.eval()\r\n","\r\n","  randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n","\r\n","  for rep in range(2):\r\n","    eval_input = encode(randword, word2index).cuda()\r\n","    print(\"INPUT: \" + string + \" \" + randword)\r\n","\r\n","    for i in range(50):\r\n","      \r\n","      output = model(eval_input)\r\n","      pred = output.softmax(-1).argmax(-1)\r\n","\r\n","      eval_input = torch.cat((eval_input,pred[-1].unsqueeze(0).unsqueeze(0)),1)\r\n","      \r\n","      if (word2index['END'] in eval_input[0]):\r\n","        break\r\n","\r\n","    print(\" \".join(decode(eval_input.tolist()[0],index2word)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GfHtYA-yPGnY"},"source":["for epoch in range(1001):\r\n","  model.train()\r\n","\r\n","  for input, target in zip(dataset_train, dataset_target):\r\n","    optimizer.zero_grad()\r\n","    output = model(input.cuda())\r\n","    loss = criterion(output, target.view(-1).cuda())\r\n","    loss.backward()\r\n","    optimizer.step()\r\n","    break\r\n","\r\n","  print(\"[{:02d}/1001] {:.4f} \".format(epoch+1, loss))\r\n","  pred = output.softmax(-1).argmax(-1)\r\n","  print(\" \".join([index2word[int(input[0][0])]] + decode(pred.tolist(),index2word)))\r\n","\r\n","  if (epoch % 5 == 0):\r\n","    #torch.save(model.state_dict(), \"drive/MyDrive/Data/\" + \"Checkpoint-\" + str(epoch) )\r\n","    torch.save(\r\n","        {'model_state_dict': model.state_dict(),\r\n","         'optimizer_state_dict': optimizer.state_dict(),},\r\n","        'drive/MyDrive/Data/Checkpoint/' + \"CPOINT-\" + str(epoch)\r\n","               )\r\n","\r\n","  print(\"===========================================================================\")\r\n","\r\n","  test_model(model, word2index, index2word)\r\n","  test_model(model, word2index, index2word, \"i am plain simple garak\")\r\n","\r\n","  print(\"===========================================================================\")\r\n","\r\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cybXXdJGPGur"},"source":[""],"execution_count":null,"outputs":[]}]}