{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlainSimpleGarak.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOsnqr6kCWmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc6308c-5e22-4253-9d81-5ff86b3d7a70"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "HOME = 'drive/MyDrive'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjqtb-uCWuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2171e1dc-00e1-42e8-b121-d3a382d950a7"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brc2YaVkCW0X"
      },
      "source": [
        "dataset_quotes = []\r\n",
        "\r\n",
        "import os\r\n",
        "with open(\"drive/MyDrive/Data/PlainSimpleGarak-data.txt\", 'r') as f: # open in readonly mode\r\n",
        "  while True:\r\n",
        "    line = f.readline().strip()\r\n",
        "    if line == '':\r\n",
        "        break\r\n",
        "    else:\r\n",
        "      dataset_quotes.append(line)\r\n",
        "\r\n",
        "dataset_startwords = []\r\n",
        "for s in dataset_quotes:\r\n",
        "  dataset_startwords.append(nltk.word_tokenize(s)[0].lower())\r\n",
        "\r\n",
        "dataset_raw = pd.read_csv('drive/MyDrive/Data/TNG.csv', error_bad_lines=False, engine='python')\r\n",
        "\r\n",
        "dataset_quotes_nonascii = list(dataset_raw[dataset_raw['type']==\"speech\"][\"text\"])\r\n",
        "\r\n",
        "for s in dataset_quotes_nonascii:\r\n",
        "  if (len(s) == len(s.encode())):\r\n",
        "    if (\"$\" not in s and \"&\" not in s):\r\n",
        "      dataset_quotes.append(s)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzrtEhX9CW7z"
      },
      "source": [
        "def preprocess_txt(dataset_quotes, append=True):\r\n",
        "  for i in range(len(dataset_quotes)):\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].lower()\r\n",
        "    if (append == True):\r\n",
        "      dataset_quotes[i] = dataset_quotes[i] + \" END\"\r\n",
        "\r\n",
        "preprocess_txt(dataset_quotes)\r\n",
        "preprocess_txt(dataset_startwords, False)\r\n",
        "\r\n",
        "#dataset_quotes.sort()\r\n",
        "dataset_startwords = list(set(dataset_startwords))\r\n",
        "#dataset_startwords.sort()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djk2Wm4ECXNX"
      },
      "source": [
        "def encode(string, word2index):\r\n",
        "  return torch.LongTensor([[word2index[wd] for wd in nltk.word_tokenize(string)]])\r\n",
        "\r\n",
        "def decode(vec, index2word):\r\n",
        "  return [index2word.get(x) for x in vec]\r\n",
        "\r\n",
        "class Dataset(torch.utils.data.Dataset):\r\n",
        "  def __init__(self, txt, seq_len, word2index):\r\n",
        "    self.encoded = [word2index[wd] for wd in txt]\r\n",
        "    self.seq_len = seq_len\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.encoded) - self.seq_len\r\n",
        "\r\n",
        "  def __getitem__(self, index):\r\n",
        "    return ( torch.tensor(self.encoded[index:index+self.seq_len]), torch.tensor(self.encoded[index+1:index+self.seq_len+1]) )\r\n",
        "\r\n",
        "txt_quotes = \" \".join(dataset_quotes)\r\n",
        "txt_quotes = nltk.word_tokenize(txt_quotes)\r\n",
        "\r\n",
        "list_words = list(set(txt_quotes))\r\n",
        "list_words.sort()\r\n",
        "\r\n",
        "word2index = {tkn: i for i, tkn in enumerate(list_words, 1)}\r\n",
        "word2index['UNKNOWN']=0\r\n",
        "index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-aKVYk9VbNf"
      },
      "source": [
        "class Net_variant(nn.Module):\r\n",
        "  def __init__(self, embed_size, input_dim, hidden_dim, batch_first=True, n_layers = 1, dropout = 0.2):\r\n",
        "    super(Net_variant, self).__init__()\r\n",
        "\r\n",
        "    self.n_layers = n_layers #unused\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "    #shared embedding layer\r\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=embed_size, embedding_dim=input_dim)\r\n",
        "    \r\n",
        "    #1\r\n",
        "    self.rnn_layer1 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first, num_layers=n_layers, dropout=dropout, bidirectional=True)\r\n",
        "    self.linear1 = nn.Linear(hidden_dim, embed_size)\r\n",
        "\r\n",
        "    #2\r\n",
        "    self.rnn_layer2 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first, num_layers=n_layers, dropout=dropout, bidirectional=True)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim, embed_size)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.embedding_layer(x)\r\n",
        "\r\n",
        "    if (random.randrange(2) == 0):\r\n",
        "      output1, hidden1 = self.rnn_layer1(output)\r\n",
        "      output1 = self.linear1(output1)\r\n",
        "      return output1\r\n",
        "    else:\r\n",
        "      output2, hidden2 = self.rnn_layer2(output)\r\n",
        "      output2 = self.linear2(output2)\r\n",
        "      return output2\r\n",
        "\r\n",
        "    #return output\r\n",
        "    #return output.view(-1, output.size(2))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTuY0mFsPGJM"
      },
      "source": [
        "vocab_size = len(word2index)\r\n",
        "input_size =  128\r\n",
        "hidden_size = 128"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIrDNq1PGbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efac9db-2d0b-4ace-e98a-4bb6d9a20aba"
      },
      "source": [
        "model = Net_variant(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=model.parameters())\r\n",
        "\r\n",
        "dataset = Dataset(txt_quotes, 10, word2index)\r\n",
        "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3i1JU-yCaUM"
      },
      "source": [
        "epoch_to_load = 0\r\n",
        "\r\n",
        "if epoch_to_load != 0:\r\n",
        "  x = torch.load(\"drive/MyDrive/Data/Checkpoint1/\" + \"CPOINT-\" + str(epoch_to_load))\r\n",
        "  model.load_state_dict(x['model_state_dict'])\r\n",
        "  optimizer.load_state_dict(x['optimizer_state_dict'])\r\n",
        "  epoch_to_load = epoch_to_load + 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ifOlYWPGhV"
      },
      "source": [
        "def test_model(model, word2index, index2word, string=\"\", maxlen=25, verbose=False):\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  eval_input = encode(string, word2index).cuda()\r\n",
        "  print(\"INITIAL INPUT: \" + string)\r\n",
        "\r\n",
        "  if verbose:\r\n",
        "    print(\"---\")\r\n",
        "\r\n",
        "  for i in range(maxlen):\r\n",
        "    output = model(eval_input)\r\n",
        "    pred = output.softmax(-1).argmax(-1)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "      print(\"INPUT: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "      print(\"OUTPUT: \" + \" \".join( decode(pred[0].tolist(), index2word)))\r\n",
        "\r\n",
        "    eval_input = torch.cat((eval_input,pred[:,-1].unsqueeze(0)), 1)\r\n",
        "\r\n",
        "    if word2index['END'] in eval_input:\r\n",
        "      break\r\n",
        "\r\n",
        "  print(\"GENERATED SEQUENCE: \" + \" \".join( decode(eval_input.tolist()[0],index2word)))\r\n",
        "  print(\"\")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53zDdnk5m_83",
        "outputId": "2b7487e1-42ed-4267-b113-854ba0ab7adc"
      },
      "source": [
        "for epoch in range(epoch_to_load, 101):\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for batch, (input, target) in enumerate(dataloader):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    output = model(input.cuda())\r\n",
        "    loss = criterion(output.transpose(1, 2), target.cuda())\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "  print(\"Epoch {:02d} / 100 Loss {:.4f}\".format(epoch+1, loss))\r\n",
        "  \r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"---------------------------------------------------------------------------\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "  test_model(model, word2index, index2word, randword)\r\n",
        "  test_model(model, word2index, index2word, randword)\r\n",
        "\r\n",
        "  test_model(model, word2index, index2word, \"i am plain simple garak . \" + randword)\r\n",
        "  test_model(model, word2index, index2word, \"i am plain simple garak . \" + randword)\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  if (epoch % 5 == 0):\r\n",
        "    #torch.save(model.state_dict(), \"drive/MyDrive/Data/\" + \"Checkpoint-\" + str(epoch) )\r\n",
        "    torch.save(\r\n",
        "        {'model_state_dict': model.state_dict(),\r\n",
        "         'optimizer_state_dict': optimizer.state_dict(),},\r\n",
        "        'drive/MyDrive/Data/Checkpoint1/' + \"CPOINT-\" + str(epoch)\r\n",
        "               )\r\n",
        "\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 01 / 100 Loss 4.0404\n",
            "===========================================================================\n",
            "TARGET: . END we 've found survivors , sir.. they 're\n",
            "INPUT: worf . END we 've found survivors , sir.. they\n",
            "PREDICTION: . END i 're been a . sir . 've\n",
            "---------------------------------------------------------------------------\n",
            "TARGET: . END we 've found survivors , sir.. they 're\n",
            "INPUT: worf . END we 've found survivors , sir.. they\n",
            "PREDICTION: . END i 're been a . sir . 've\n",
            "===========================================================================\n",
            "INITIAL INPUT: our\n",
            "GENERATED SEQUENCE: our sensors are still in the vicinity of the federation . END\n",
            "\n",
            "INITIAL INPUT: our\n",
            "GENERATED SEQUENCE: our sensors , sir . END\n",
            "\n",
            "INITIAL INPUT: i am plain simple garak . our\n",
            "GENERATED SEQUENCE: i am plain simple garak . our sensors are picking up a small message from the planet . END\n",
            "\n",
            "INITIAL INPUT: i am plain simple garak . our\n",
            "GENERATED SEQUENCE: i am plain simple garak . our sensors are picking up a small pocket of the federation . END\n",
            "\n",
            "===========================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZszbIxhSnFF8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfHtYA-yPGnY"
      },
      "source": [
        "dataset_quotes = []\r\n",
        "with open(\"drive/MyDrive/Data/PlainSimpleGarak-data.txt\", 'r') as f: # open in readonly mode\r\n",
        "  while True:\r\n",
        "    line = f.readline().strip()\r\n",
        "    if line == '':\r\n",
        "        break\r\n",
        "    else:\r\n",
        "      dataset_quotes.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzkWUg-bLz5N"
      },
      "source": [
        "preprocess_txt(dataset_quotes)\r\n",
        "preprocess_txt(dataset_startwords, False)\r\n",
        "\r\n",
        "dataset_startwords = list(set(dataset_startwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtlJK8P-Mknd"
      },
      "source": [
        "txt_quotes = \" \".join(dataset_quotes)\r\n",
        "txt_quotes = nltk.word_tokenize(txt_quotes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47r5zdi0NgY7"
      },
      "source": [
        "model = Net_variant(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.001)\r\n",
        "\r\n",
        "dataset = Dataset(txt_quotes, 10, word2index)\r\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEkpUobLNiCd"
      },
      "source": [
        "epoch_to_load = 100\r\n",
        "\r\n",
        "if epoch_to_load != 0:\r\n",
        "  x = torch.load(\"drive/MyDrive/Data/Checkpoint1/\" + \"CPOINTB-\" + str(epoch_to_load))\r\n",
        "  model.load_state_dict(x['model_state_dict'])\r\n",
        "  #optimizer.load_state_dict(x['optimizer_state_dict'])\r\n",
        "  epoch_to_load = epoch_to_load + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVfnVjN1NiIj"
      },
      "source": [
        "randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "test_model(model, word2index, index2word, \"good doctor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdroP3n6NiOe"
      },
      "source": [
        "for epoch in range(epoch_to_load, 126):\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for batch, (input, target) in enumerate(dataloader):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    output = model(input.cuda())\r\n",
        "    loss = criterion(output.transpose(1, 2), target.cuda())\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "  print(\"Epoch {:02d} / 125 Loss {:.4f}\".format(epoch+1, loss))\r\n",
        "  \r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"---------------------------------------------------------------------------\")\r\n",
        "\r\n",
        "  print(\"TARGET: \" + \" \".join( decode(target[0].tolist(),index2word)))\r\n",
        "  print(\"INPUT: \" + \" \".join( decode(input[0].tolist(),index2word)))\r\n",
        "  pred = output[0].softmax(-1).argmax(-1)\r\n",
        "  print(\"PREDICTION: \" + \" \".join(decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "  test_model(model, word2index, index2word, randword)\r\n",
        "  test_model(model, word2index, index2word, randword)\r\n",
        "\r\n",
        "  test_model(model, word2index, index2word, \"i am plain simple garak . \" + randword)\r\n",
        "  test_model(model, word2index, index2word, \"i am plain simple garak . \" + randword)\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  if (epoch % 50 == 0):\r\n",
        "    #torch.save(model.state_dict(), \"drive/MyDrive/Data/\" + \"Checkpoint-\" + str(epoch) )\r\n",
        "    torch.save(\r\n",
        "        {'model_state_dict': model.state_dict(),\r\n",
        "         'optimizer_state_dict': optimizer.state_dict(),},\r\n",
        "        'drive/MyDrive/Data/Checkpoint1/' + \"CPOINT_FINETUNE-\" + str(epoch)\r\n",
        "               )\r\n",
        "\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUpeBOTVNiVP"
      },
      "source": [
        "epoch_to_load = 125\r\n",
        "\r\n",
        "if epoch_to_load != 0:\r\n",
        "  x = torch.load(\"drive/MyDrive/Data/Checkpoint1/\" + \"CPOINT_FINETUNE-\" + str(epoch_to_load))\r\n",
        "  model.load_state_dict(x['model_state_dict'])\r\n",
        "  #optimizer.load_state_dict(x['optimizer_state_dict'])\r\n",
        "  epoch_to_load = epoch_to_load + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjepplBMNicB"
      },
      "source": [
        "randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "test_model(model, word2index, index2word, \"doctor bashir lying is a skill .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87vuqyKuNiib"
      },
      "source": [
        "test_model(model, word2index, index2word, \"i am plain simple garak .\")\r\n",
        "test_model(model, word2index, index2word, \"i am plain simple garak .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctpq8D20Nisk"
      },
      "source": [
        "test_model(model, word2index, index2word, \"i'm not a spy doctor .\")\r\n",
        "test_model(model, word2index, index2word, \"i'm not a spy doctor .\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIlycosXrBiy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Fc8r-YmrLuN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvXMPQ9rNJ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC-2VTBYrRj8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSSMpCPtrYEZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUKlKby4MePX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMD2J5XPr11k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUtVLr_2LAMT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR4yGzIlYkPb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7I6J_vYkXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxdecADjYkkc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}