{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PlainSimpleGarak.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOsnqr6kCWmk"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "HOME = 'drive/MyDrive'\r\n",
        "!pip install torchtext==0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTjqtb-uCWuJ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "import random\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brc2YaVkCW0X"
      },
      "source": [
        "dataset_quotes = []\r\n",
        "\r\n",
        "import os\r\n",
        "with open(\"drive/MyDrive/Data/PlainSimpleGarak-data.txt\", 'r') as f: # open in readonly mode\r\n",
        "  while True:\r\n",
        "    line = f.readline().strip()\r\n",
        "    if line == '':\r\n",
        "        break\r\n",
        "    else:\r\n",
        "      dataset_quotes.append(line)\r\n",
        "\r\n",
        "dataset_startwords = []\r\n",
        "\r\n",
        "\r\n",
        "for s in dataset_quotes:\r\n",
        "  dataset_startwords.append(s.split()[0].lower())\r\n",
        "\r\n",
        "dataset_raw = pd.read_csv('drive/MyDrive/Data/quotes.csv', error_bad_lines=False, engine='python')\r\n",
        "\r\n",
        "dataset_quotes_nonascii = list(dataset_raw[\"Quote\"] )\r\n",
        "\r\n",
        "for s in dataset_quotes_nonascii:\r\n",
        "  if (len(s) == len(s.encode())):\r\n",
        "    if (\"$\" not in s and \"&\" not in s):\r\n",
        "      dataset_quotes.append(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzrtEhX9CW7z"
      },
      "source": [
        "def preprocess_txt(dataset_quotes, appende=True):\r\n",
        "  for i in range(len(dataset_quotes)):\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('\\t', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('--', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(' - ', ' ')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('%', ' percent')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('[', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(']', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(':', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(';', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('?', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('#', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('!', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('/', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('.', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(',', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('(', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace(')', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('*', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('+', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].replace('\\\"', '')\r\n",
        "    #dataset_quotes[i] = dataset_quotes[i].replace('\\'', '')\r\n",
        "    dataset_quotes[i] = dataset_quotes[i].lower()\r\n",
        "    if (appende == True):\r\n",
        "      dataset_quotes[i] = dataset_quotes[i] + \" END\"\r\n",
        "\r\n",
        "preprocess_txt(dataset_quotes)\r\n",
        "preprocess_txt(dataset_startwords, False)\r\n",
        "\r\n",
        "dataset_startwords = list(set(dataset_startwords))\r\n",
        "\r\n",
        "random.shuffle(dataset_quotes)\r\n",
        "random.shuffle(dataset_startwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djk2Wm4ECXNX"
      },
      "source": [
        "txt_quotes = \"\"\r\n",
        "for s in dataset_quotes:\r\n",
        "  txt_quotes = txt_quotes + \" \" + s\r\n",
        "txt_quotes = txt_quotes.split()\r\n",
        "\r\n",
        "list_words = list(set(txt_quotes))\r\n",
        "word2index = {tkn: i for i, tkn in enumerate(list_words, 1)}\r\n",
        "word2index['<UNKNOWN>']=0\r\n",
        "index2word = {v: k for k, v in word2index.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPYA4ABfCXVU"
      },
      "source": [
        "def string_to_trainpair(string, word2index):\r\n",
        "  string = string.split()\r\n",
        "  encoded = [word2index[wd] for wd in string]\r\n",
        "  input_str, target_str = encoded[:-1], encoded[1:]\r\n",
        "  input_str = torch.LongTensor(input_str).unsqueeze(0)\r\n",
        "  target_str = torch.LongTensor(target_str).unsqueeze(0)\r\n",
        "  return input_str, target_str\r\n",
        "\r\n",
        "def encode(string, word2index):\r\n",
        "  return torch.LongTensor([[word2index[token] for token in string.split()]])\r\n",
        "\r\n",
        "def decode(vec, index2word):\r\n",
        "  return [index2word.get(x) for x in vec]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3s8w7E_CXcf"
      },
      "source": [
        "dataset_train = []\r\n",
        "dataset_target = []\r\n",
        "\r\n",
        "for s in dataset_quotes:\r\n",
        "  input, target = string_to_trainpair(s, word2index)\r\n",
        "  dataset_train.append(input)\r\n",
        "  dataset_target.append(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1vR2qyCCXkR"
      },
      "source": [
        "class Net(nn.Module):\r\n",
        "  def __init__(self, embed_size, input_dim, hidden_dim, batch_first=True):\r\n",
        "    super(Net, self).__init__()\r\n",
        "\r\n",
        "    self.n_layers = 1 #unused\r\n",
        "    self.hidden_dim = hidden_dim\r\n",
        "\r\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=embed_size, embedding_dim=input_dim)\r\n",
        "    #1\r\n",
        "    self.rnn_layer1 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first)\r\n",
        "    self.linear1 = nn.Linear(hidden_dim, embed_size)\r\n",
        "    #2\r\n",
        "    self.rnn_layer2 = nn.GRU(input_dim, hidden_dim, batch_first=batch_first)\r\n",
        "    self.linear2 = nn.Linear(hidden_dim, embed_size)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    output = self.embedding_layer(x)\r\n",
        "\r\n",
        "    if self.training:\r\n",
        "      output_1, hidden_1 = self.rnn_layer1(output)\r\n",
        "      output_1 = self.linear1(output_1)\r\n",
        "      output_2, hidden_2 = self.rnn_layer2(output)\r\n",
        "      output_2 = self.linear2(output_2)\r\n",
        "\r\n",
        "      output = 0.5 * (output_1 + output_2)\r\n",
        "\r\n",
        "    else:\r\n",
        "      if (random.randrange(2) == 0):\r\n",
        "        output, hidden = self.rnn_layer1(output)\r\n",
        "        output = self.linear1(output)\r\n",
        "      else:\r\n",
        "        output, hidden = self.rnn_layer2(output)\r\n",
        "        output = self.linear2(output)\r\n",
        "    return output.view(-1, output.size(2))\r\n",
        "\r\n",
        "  def init_hidden(self, batch_size):\r\n",
        "      return torch.zeros(self.n_layers, batch_size, self.hidden_dim)  #unused"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTuY0mFsPGJM"
      },
      "source": [
        "vocab_size = len(word2index)\r\n",
        "input_size =  256\r\n",
        "hidden_size = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkIrDNq1PGbB"
      },
      "source": [
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\r\n",
        "model.cuda()\r\n",
        "\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6ifOlYWPGhV"
      },
      "source": [
        "def test_model(model, word2index, index2word, string=\"\"):\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  randword = dataset_startwords[random.randrange(0,len(dataset_startwords))]\r\n",
        "\r\n",
        "  for rep in range(2):\r\n",
        "    eval_input = encode(randword, word2index).cuda()\r\n",
        "    print(\"INPUT: \" + string + \" \" + randword)\r\n",
        "\r\n",
        "    for i in range(50):\r\n",
        "      \r\n",
        "      output = model(eval_input)\r\n",
        "      pred = output.softmax(-1).argmax(-1)\r\n",
        "\r\n",
        "      eval_input = torch.cat((eval_input,pred[-1].unsqueeze(0).unsqueeze(0)),1)\r\n",
        "      \r\n",
        "      if (word2index['END'] in eval_input[0]):\r\n",
        "        break\r\n",
        "\r\n",
        "    print(\" \".join(decode(eval_input.tolist()[0],index2word)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfHtYA-yPGnY"
      },
      "source": [
        "for epoch in range(1001):\r\n",
        "  model.train()\r\n",
        "\r\n",
        "  for input, target in zip(dataset_train, dataset_target):\r\n",
        "    optimizer.zero_grad()\r\n",
        "    output = model(input.cuda())\r\n",
        "    loss = criterion(output, target.view(-1).cuda())\r\n",
        "    loss.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "  print(\"[{:02d}/1001] {:.4f} \".format(epoch+1, loss))\r\n",
        "  pred = output.softmax(-1).argmax(-1)\r\n",
        "  print(\" \".join([index2word[int(input[0][0])]] + decode(pred.tolist(),index2word)))\r\n",
        "\r\n",
        "  if (epoch % 5 == 0):\r\n",
        "    #torch.save(model.state_dict(), \"drive/MyDrive/Data/\" + \"Checkpoint-\" + str(epoch) )\r\n",
        "    torch.save(\r\n",
        "        {'model_state_dict': model.state_dict(),\r\n",
        "         'optimizer_state_dict': optimizer.state_dict(),},\r\n",
        "        'drive/MyDrive/Data/Checkpoint/' + \"CPOINT-\" + str(epoch)\r\n",
        "               )\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  test_model(model, word2index, index2word)\r\n",
        "  test_model(model, word2index, index2word, \"i am plain simple garak\")\r\n",
        "\r\n",
        "  print(\"===========================================================================\")\r\n",
        "\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cybXXdJGPGur"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}